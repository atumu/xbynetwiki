title: 分词器介绍 

#  lucene:分词器介绍 

参考：http://blog.csdn.net/xiaojimanman/article/details/42916755
在lucene创建索引的过程中，数据信息的处理是一个十分重要的过程，在这一过程中，主要的部分就是这一篇博客的主题：分词器。在下面简单的demo中，介绍了7中比较常见的分词技术，即：CJKAnalyzer、KeywordAnalyzer、SimpleAnalyzer、StopAnalyzer、WhitespaceAnalyzer、StandardAnalyzer、IKAnalyzer；对于中文分词，最新版本有一个叫做smartcn扩展lucene-analyzers-smartcn其中有一个SmartChineseAnalyzer。以下为API文档中的对比：可见其效率还可以。
 Example phrase： "我是中国人"
  * StandardAnalyzer: 我－是－中－国－人
  * CJKAnalyzer: 我是－是中－中国－国人
  * SmartChineseAnalyzer: 我－是－中国－人
自己可以通过注释的形式一一验证。源程序如下：

#  Analyzer分词demo 

```

import java.io.StringReader;  
import org.apache.lucene.analysis.cn.smart.SmartChineseAnalyzer  
import org.apache.lucene.analysis.Analyzer;  
import org.apache.lucene.analysis.TokenStream;  
import org.apache.lucene.analysis.cjk.CJKAnalyzer;  
import org.apache.lucene.analysis.core.KeywordAnalyzer;  
import org.apache.lucene.analysis.core.SimpleAnalyzer;  
import org.apache.lucene.analysis.core.StopAnalyzer;  
import org.apache.lucene.analysis.core.WhitespaceAnalyzer;  
import org.apache.lucene.analysis.standard.StandardAnalyzer;  
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;  
import org.apache.lucene.util.Version;  
import org.wltea.analyzer.lucene.IKAnalyzer;  
public class AnalyzerStudy {  
  
    public static void main(String[] args) throws Exception {  
        //需要处理的测试字符串  
        String str = "这是一个分词器测试程序，希望大家继续关注我的个人系列博客：基于Lucene的案例开发，这里加一点带空格的标签 LUCENE java 分词器";  
        Analyzer analyzer = null;  
      
        //标准分词器，如果用来处理中文，和ChineseAnalyzer有一样的效果，这也许就是之后的版本弃用ChineseAnalyzer的一个原因  
        analyzer = new StandardAnalyzer(Version.LUCENE_43);  
      	//使用自带的中文分词扩展包lucene-analyzers.smartcn.jar:
       analyzer=new SmartChineseAnalyzer();
        //第三方中文分词器，有下面2中构造方法。  
        analyzer = new IKAnalyzer();  
        analyzer = new IKAnalyzer(false);  
        analyzer = new IKAnalyzer(true);  
        //空格分词器，对字符串不做如何处理  
        analyzer = new WhitespaceAnalyzer(Version.LUCENE_43);  
        //简单分词器，一段一段话进行分词  
        analyzer = new SimpleAnalyzer(Version.LUCENE_43);  
        //二分法分词器，这个分词方式是正向退一分词(二分法分词)，同一个字会和它的左边和右边组合成一个次，每个人出现两次，除了首字和末字  
        analyzer = new CJKAnalyzer(Version.LUCENE_43);  
        //关键字分词器，把处理的字符串当作一个整体  
        analyzer = new KeywordAnalyzer();  
        //被忽略的词分词器  
        analyzer = new StopAnalyzer(Version.LUCENE_43);  
          
        //使用分词器处理测试字符串  
        StringReader reader = new StringReader(str);  
        TokenStream  tokenStream  = analyzer.tokenStream("", reader);  
        tokenStream.reset();  
        CharTermAttribute  term = tokenStream.getAttribute(CharTermAttribute.class);  
        int l = 0;  
        //输出分词器和处理结果  
        System.out.println(analyzer.getClass());  
        while(tokenStream.incrementToken()){    
            System.out.print(term.toString() + "|");  
            l += term.toString().length();  
            //如果一行输出的字数大于30，就换行输出  
            if (l > 30) {  
                System.out.println();  
                l = 0;  
            }  
        }   
    }  
}  

```
#  分词器介绍 
##  StandardAnalyzer 

StandardAnalyzer标准分词器，如果用来处理中文，和ChineseAnalyzer有一样的效果，这也许就是之后的版本弃用ChineseAnalyzer的一个原因。` 用StandardAnalyzer处理英文效果还不错，但是对中文的处理只是将其分成单个汉字，并不存在任何语义或词性，如果实在没有其他的分词器，用StandardAnalyzer来处理中文还是可以的， `上述事例使用StandardAnalyzer分词技术的运行结果如下图：
![](/data/dokuwiki/opensourcelearn/lucene/pasted/20150512-190247.png)
##  IKAnalyzer 
项目地址：http://git.oschina.net/wltea/IK-Analyzer-2012FF/tree/master
```

<!-- oschina maven私服-->
<dependency>
  <groupId>IKAnalyzer</groupId>
  <artifactId>IKAnalyzer</artifactId>
  <version>2012FF_u1</version>
</dependency>

```
IK Analyzer 2012版本支持** 细粒度切分 和 智能切分**，以下是两种切分方式的演示样例。
张三说的确实在理
 智能分词结果:
张三 | 说的 | 确实 | 在理
 最细粒度分词结果:
张三 | 三 | 说的 | 的确 | 的 | 确实 | 实在 | 在理

Ikanalyzer构造函数分析：
public IKAnalyzer()
说明：构造函数，默认实现最细粒度切分算法
public IKAnalyzer(boolean useSmart)
说明：构造函数参数1 ：boolean useSmart， 当为true时，分词器采用智能切分 ；当为false时，分词器迚行最细粒度切分。
###  IKAnalyzer安装部署 
  * 将IKAnalyzer2012.jar部署亍项目的lib目彔中；
  * IKAnalyzer.cfg.xml与stopword.dic文件放置在class根目彔（对亍web项目，通常是WEB-INF/classes目彔，同hibernate、log4j等配置文件相同）下即可。

 IKAnalyzer是基于Lucene的第三方中文分词技术，该分词技术基于现有的中文词库实现的，在构造Analyzer对象时有两种构造方法，无参构造等同于new IKAnalyzer(false) ，在介绍true/false两种参数下分词器的不同之前先看看这两种情况下的事例运行结果：

false运行结果如下图：
![](/data/dokuwiki/opensourcelearn/lucene/pasted/20150512-190338.png)
true运行结果如下图：
![](/data/dokuwiki/opensourcelearn/lucene/pasted/20150512-190347.png)

从上述事例中，我们可以简单的看出，false的情况下会对已分的词进行再分，如果存在长度较小的词元，也将其作为一个分词结果。` IKAnalyzer是一种比较常用的中文分词技术。在2012版本中，IK实现了简单的分词歧义排除算法，标志着IK分词器从单纯的词典分词向模拟语义分词衍化。 `
##  WhitespaceAnalyzer 

WhitespaceAnalyzer空格分词，这个分词技术就相当于按照空格简单的切分字符串，对形成的子串不做其他的操作，结果同string.split(" ")的结果类似。上述事例在WhitespaceAnalyzer分词技术下的运行结果如下图：
![](/data/dokuwiki/opensourcelearn/lucene/pasted/20150512-190500.png)
 这种分词技术也许你会绝对没有太大的作用，它对输入的字符串几乎没有做太多的处理，对语句的处理结果也不是太好，如果这样想就错了，下面就简单的想一下这个问题，这篇博客的标签是 lucene、java、分词器，那这三个词在索引中又该如何的存储，采用何种分词技术呢？这里不做任何解答，自己思考下，在以后的小说案例中会对标签这个域提出具体的解决方案。
##  SimpleAnalyzer 
SimpleAnalyzer简单分词器，与其说是一段话进行分词，不如说是一句话就是一个词，遇到标点、空格等，就将其之前的内容当作一个词。上述事例在SimpleAnalyzer分词技术下的运行结果如下图：
![](/data/dokuwiki/opensourcelearn/lucene/pasted/20150512-190611.png)
##  CJKAnalyzer 

CJKAnalyzer是二分法分词器，这个分词方式是正向退一分词(二分法分词)，同一个字会和它的左边和右边组合成一个次，每个人出现两次，除了首字和末字，` 也就是说会将任何两个相邻的汉字当作是一个词，这种分词技术会产生大量的无用词组 `。上述事例在CJKAnalyzer分词技术下的运行结果如下图：
![](/data/dokuwiki/opensourcelearn/lucene/pasted/20150512-190716.png)
##  KeywordAnalyzer 

KeywordAnalyzer关键字分词器，**把处理的字符串当作一个整体**，这个分词器，在lucene之前的版本中或许还有点作用，**但最近的几个版本中，Lucene对域的类型做了细分，它的作用就不是太大了**，不做在luke中，还是相当重要的。上述事例在KeywordAnalyzer分词技术下的运行结果如下图：
![](/data/dokuwiki/opensourcelearn/lucene/pasted/20150512-190814.png)
##  StopAnalyzer 

 StopAnalyzer被忽略的词分词器，被忽略的词就是在分词结果中，被丢弃的字符串，如标点、空格等。上述事例在StopAnalyzer分词技术下的运行结果如下图：
![](/data/dokuwiki/opensourcelearn/lucene/pasted/20150512-190928.png)

上述的7种分词技术都可以对中文做处理，**对外文（非英语）**的处理有以下几种分词技术：

BrazilianAnalyzer 巴西语言分词 
CzechAnalyzer 捷克语言分词
DutchAnalyzer 荷兰语言分词
FrenchAnalyzer 法国语言分词
GermanAnalyzer 德国语言分词
GreekAnalyzer 希腊语言分词
RussianAnalyzer 俄罗斯语言分词
ThaiAnalyzer 泰国语言分词